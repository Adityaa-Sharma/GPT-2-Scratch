{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text length: 111\n",
      "Vocabulary size: 111\n",
      "First 10 characters: ['\\t', '\\n', ' ', 'A', 'B', 'C', 'D', 'E', 'F', 'G']\n",
      "['\\t', '\\n', ' ', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '\\xa0', '²', '³', '¹', '¾', 'À', 'Á', 'Ä', 'Æ', 'È', 'É', 'Ñ', 'Ô', 'Õ', 'Ü', 'Ý', 'ß', 'à', 'á', 'â', 'ã', 'ä', 'å', 'æ', 'ç', 'è', 'é', 'ê', 'ë', 'ì', 'í', 'î', 'ï', 'ð', 'ñ', 'ò', 'ó', 'ô', 'õ', 'ö', 'ù', 'ú', 'û', 'ü', 'ý', 'ÿ', 'Œ', 'œ', 'š', 'Ž', 'ž', 'ל', 'מ', 'ת', '饟']\n",
      "Tokens: [10, 34, 41, 41, 44, 2, 25, 44, 47, 41, 33]\n",
      "Decoded: Hello World\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from Tokenization.tokenization import CharacterTokenization\n",
    "from TextFormatter.formatter import CharacterFormatter\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "# Load file\n",
    "vocab_file = r\"C:\\Users\\91978\\Desktop\\GPT-2-Scratch\\dataset\\Poems.txt\"\n",
    "with open(vocab_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "formatter = CharacterFormatter(text)\n",
    "text = formatter.preprocess()\n",
    "\n",
    "print(\"Original text length:\", len(text))\n",
    "\n",
    "# # Clean text before creating vocabulary\n",
    "# text = re.sub(r'\\d+', '', text)\n",
    "# text = re.sub(r'[^\\w\\s]', '', text)  # remove special characters\n",
    "# #remove greek characters\n",
    "# text = re.sub(r'[α-ωΑ-Ω]', '', text)\n",
    "\n",
    "vocab = sorted(list(set(text)))\n",
    "print(\"Vocabulary size:\", len(vocab))\n",
    "print(\"First 10 characters:\", vocab[:10])\n",
    "print(vocab)\n",
    "\n",
    "# Initialize tokenizer with clean vocab\n",
    "tokenizer = CharacterTokenization(vocab)\n",
    "\n",
    "# Test tokenization\n",
    "test_text = \"Hello World\"\n",
    "tokens = tokenizer.tokenize(test_text)\n",
    "print(\"Tokens:\", tokens)\n",
    "decoded=tokenizer.detokenize(tokens)\n",
    "print(\"Decoded:\", decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text length: 35370769\n",
      "Vocabulary size: 146882\n",
      "Preprocessed text length: ['ABBOT', 'ABC', 'ABNER', 'ABOARD', 'ABOLISHING', 'ABOUT', 'ABOVE', 'ABRAHAM', 'ABSENCE', 'ABSTRACTEDLY', 'AC', 'ACCENTS', 'ACCIDENT', 'ACCORD', 'ACCOST', 'ACCOUNT', 'ACHIHA', 'ACT', 'ACTO', 'ACTRESS', 'ACTS', 'AD', 'ADDRESSED', 'ADDRESSES', 'ADELE', 'ADIEU', 'ADIEUS', 'ADMIR', 'ADMIRATION', 'ADMIRE', 'ADMISSION', 'ADMIT', 'ADMITTANCE', 'ADONIS', 'ADOR', 'ADORATION', 'ADVERSITY', 'AE', 'AENEIDEM', 'AENEIDS', 'AETERNAL', 'AEUM', 'AEgis', 'AEneas', 'AEschylean', 'AEschylos', 'AEsop', 'AEther', 'AEtna', 'AFAR', 'AFFAIRS', 'AFFRONT', 'AFOOT', 'AFRICA', 'AFRICAN', 'AFTER', 'AFter', 'AGAIN', 'AGAINST', 'AGATHA', 'AGE', 'AGED', 'AGES', 'AGIB', 'AGLÄE', 'AGNES', 'AGONIES', 'AGWNWN', 'AH', 'AI', 'AID', 'AIDS', 'AIKIN', 'AIM', 'AIN', 'AIR', 'AIl', 'ALARM', 'ALASKAN', 'ALBANY', 'ALBATROSS', 'ALBEIT', 'ALBUM', 'ALCESTIS', 'ALCMAEON', 'ALEXANDER', 'ALEXIS', 'ALIEN', 'ALIVE', 'ALL', 'ALLEGORICAL', 'ALMORAN', 'ALMOST', 'ALONE', 'ALONG', 'ALPINE', 'ALTAR', 'ALTHOUGH', 'ALWAS', 'ALWAYS', 'ALl', 'AM', 'AMA', 'AMAR', 'AMARANTHA', 'AMARIL', 'AMBITION', 'AMBO', 'AMEN', 'AMERICA', 'AMICOS', 'AMIN', 'AMINTA', 'AMINTOR', 'AMMON', 'AMONG', 'AMONGST', 'AMOR', 'AMORE', 'AMORET', 'AMORIS', 'AMSTERDAM', 'AMY', 'AN', 'ANANDA', 'ANAXERETE', 'ANCHOR', 'ANCHORET', 'ANCIENT', 'AND', 'ANGEL', 'ANGELIC', 'ANGELICA', 'ANGELS', 'ANGLO', 'ANN', 'ANNA', 'ANNABEL', 'ANNETTA', 'ANNIVER', 'ANNIVERSARY', 'ANNUAL', 'ANOTHER', 'ANS', 'ANSWER', 'ANSWERING', 'ANT', 'ANTARTICK', 'ANTHOLOGIE', 'ANTIGONE', 'ANTIJACK', 'ANY', 'ANYBURG', 'ANYSIDE', 'ANYTHING', 'ANd', 'APATHY', 'APHRODITE', 'APOLLO', 'APOSTATE', 'APOTMON', 'APRIL', 'APRILL', 'ARABIA', 'ARABIAN', 'ARAMINTA', 'ARC', 'ARCADY', 'ARCANUM', 'ARDELIA', 'ARE', 'ARETEMIAS', 'ARGENTE', 'ARGUMENT', 'ARIADNE', 'ARIEL', 'ARK', 'ARM', 'ARMS', 'ARNO', 'AROUND', 'ARRANGING', 'ART', 'ARion', 'AS', 'ASCENT', 'ASHES', 'ASIA', 'ASINO', 'ASK', 'ASKED', 'ASS', 'ASSE', 'ASSHOLE', 'ASSYRIA', 'ASTROPHEL', 'AT', 'ATCHIEVE', 'ATE', 'ATHALIA']\n",
      "Tokens: [23463, 55298]\n",
      "Decoded: Hello World\n"
     ]
    }
   ],
   "source": [
    "from Tokenization.tokenization import WordTokenization\n",
    "from TextFormatter.formatter import CharacterFormatter, WordFormatter\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "# Load file\n",
    "vocab_file = r\"C:\\Users\\91978\\Desktop\\GPT-2-Scratch\\dataset\\Poems.txt\"\n",
    "with open(vocab_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(\"Original text length:\", len(text))\n",
    "# preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "word_formatter = WordFormatter(text)\n",
    "preprocessed = word_formatter.preprocess()  \n",
    "#remove number\n",
    "# preprocessed = [word for word in preprocessed if not word.isdigit()]\n",
    "# preprocessed = sorted(set(preprocessed))\n",
    "print(\"Vocabulary size:\", len(preprocessed))\n",
    "print(\"Preprocessed text length:\", preprocessed[:200])\n",
    "word_tokenizer = WordTokenization(preprocessed)\n",
    "test_text = \"Hello World\"\n",
    "tokens = word_tokenizer.tokenize(test_text)\n",
    "print(\"Tokens:\", tokens)\n",
    "decoded = word_tokenizer.detokenize(tokens)\n",
    "print(\"Decoded:\", decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: [15496, 2159]\n",
      "Decoded: Hello World\n"
     ]
    }
   ],
   "source": [
    "from Tokenization.tokenization import GptTokenizer\n",
    "gpt_tokenizer = GptTokenizer()\n",
    "test_text = \"Hello World\"\n",
    "tokens = gpt_tokenizer.encode(test_text)\n",
    "print(\"Tokens:\", tokens)\n",
    "decoded = gpt_tokenizer.decode(tokens)\n",
    "print(\"Decoded:\", decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
